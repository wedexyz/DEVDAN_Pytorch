original

import numpy as np
import time 
import copy
import torch
import torch.nn as nn
import torch.nn.functional as F
from numpy import linalg as LA
import pdb
from collections import deque
import random
from scipy.stats.distributions import chi2
import pandas as pd
import warnings
from utilsDEVDAN import meanStdCalculator, probitFunc, deleteRowTensor, deleteColTensor, maskingNoise
warnings.filterwarnings("ignore", category=RuntimeWarning)

class hiddenLayerBasicNet(nn.Module):
    def __init__(self, no_input, no_hidden):
        super(hiddenLayerBasicNet, self).__init__()
        # hidden layer
        self.linear = nn.Linear(no_input, no_hidden,  bias=True)
        
        self.activation1 = nn.ReLU(inplace=True)
        self.activation2 = nn.Sigmoid()
        
        nn.init.xavier_uniform_(self.linear.weight)
        self.linear.bias.data.zero_()

        # decoder
        self.biasDecoder = nn.Parameter(torch.zeros(no_input))
        
    def forward(self, x):
        x = self.linear(x)
        x = self.activation1(x)                                        # encoded output

        # decoder
        r = F.linear(x, self.linear.weight.t()) + self.biasDecoder
        r = self.activation1(r)                                        # reconstructed input
        
        return x, r

class outputLayerBasicNet(nn.Module):
    def __init__(self, no_hidden, classes):
        super(outputLayerBasicNet, self).__init__()
        
        # softmax layer
        self.linearOutput = nn.Linear(no_hidden, classes,  bias=True)
        nn.init.xavier_uniform_(self.linearOutput.weight)
        self.linearOutput.bias.data.zero_()
        
    def forward(self, x):
        x = self.linearOutput(x)
        
        return x

class hiddenLayer():
    def __init__(self, no_input, no_hidden):
        self.network = hiddenLayerBasicNet(no_input,no_hidden)
        self.netUpdateProperties()

    def netUpdateProperties(self):
        self.nNetInput   = self.network.linear.in_features
        self.nNodes      = self.network.linear.out_features
        self.nParameters = (self.network.linear.in_features*self.network.linear.out_features +
                            len(self.network.linear.bias.data) + len(self.network.biasDecoder))

    def getNetProperties(self):
        print(self.network)
        print('No. of inputs :',self.nNetInput)
        print('No. of nodes :',self.nNodes)
        print('No. of parameters :',self.nParameters)

    def getNetParameters(self):
        print('Input weight: \n', self.network.linear.weight)
        print('Input bias: \n', self.network.linear.bias)
        print('Output decoder bias: \n', self.network.biasDecoder)

    def nodeGrowing(self,nNewNode = 1):
        nNewNodeCurr = self.nNodes + nNewNode
        
        # grow node
        # newWeight, newOutputWeight,_     = generateWeightXavInit(self.nNetInput,nNewNodeCurr,self.nOutputs,nNewNode)
        newWeight                        = nn.init.xavier_uniform_(torch.empty(nNewNode, self.nNetInput))
        self.network.linear.weight.data  = torch.cat((self.network.linear.weight.data,newWeight),0)            # grow input weights
        self.network.linear.bias.data    = torch.cat((self.network.linear.bias.data,torch.zeros(nNewNode)),0)  # grow input bias
        self.network.linear.out_features = nNewNodeCurr
        del self.network.linear.weight.grad
        del self.network.linear.bias.grad
        
        self.netUpdateProperties()

    def nodePruning(self,pruneIdx,nPrunedNode = 1):
        nNewNodeCurr = self.nNodes - nPrunedNode  # prune a node
        
        # prune node for current layer, output
        self.network.linear.weight.data  = deleteRowTensor(self.network.linear.weight.data,
                                                           pruneIdx)  # prune input weights
        self.network.linear.bias.data    = deleteRowTensor(self.network.linear.bias.data,
                                                           pruneIdx)  # prune input bias
        self.network.linear.out_features = nNewNodeCurr
        del self.network.linear.weight.grad
        del self.network.linear.bias.grad
        
        self.netUpdateProperties()

    def inputGrowing(self,nNewInput = 1):
        nNewInputCurr = self.nNetInput + nNewInput

        # grow input weight
        # _,_,newWeightNext = generateWeightXavInit(nNewInputCurr,self.nNodes,self.nOutputs,nNewInput)
        newWeightNext                   = nn.init.xavier_uniform_(torch.empty(self.nNodes, nNewInput))
        self.network.linear.weight.data = torch.cat((self.network.linear.weight.data,newWeightNext),1)
        self.network.biasDecoder.data   = torch.cat((self.network.biasDecoder.data,torch.zeros(nNewInput)),0)
        
        del self.network.linear.weight.grad
        del self.network.biasDecoder.grad

        self.network.linear.in_features = nNewInputCurr
        self.netUpdateProperties()
        
    def inputPruning(self,pruneIdx,nPrunedNode = 1):
        nNewInputCurr = self.nNetInput - nPrunedNode

        # prune input weight of next layer
        self.network.linear.weight.data = deleteColTensor(self.network.linear.weight.data,pruneIdx)
        self.network.biasDecoder.data   = deleteRowTensor(self.network.biasDecoder.data,pruneIdx)
        
        del self.network.linear.weight.grad
        del self.network.biasDecoder.grad

        # update input features
        self.network.linear.in_features = nNewInputCurr
        self.netUpdateProperties()


class outputLayer():
    def __init__(self, no_hidden, classes):
        self.network = outputLayerBasicNet(no_hidden,classes)
        self.netUpdateProperties()

    def netUpdateProperties(self):
        self.nNetInput   = self.network.linearOutput.in_features
        self.nOutputs    = self.network.linearOutput.out_features
        self.nParameters = (self.network.linearOutput.in_features*self.network.linearOutput.out_features +
                            len(self.network.linearOutput.bias.data))

    def getNetProperties(self):
        print(self.network)
        print('No. of inputs :',self.nNetInput)
        print('No. of output :',self.nOutputs)
        print('No. of parameters :',self.nParameters)

    def getNetParameters(self):
        print('Output weight: \n', self.network.linearOutput.weight)
        print('Output bias: \n', self.network.linearOutput.bias)

    def inputGrowing(self,nNewInput = 1):
        nNewInputCurr = self.nNetInput + nNewInput

        # grow input weight
        # _,_,newWeightNext = generateWeightXavInit(nNewInputCurr,self.nNodes,self.nOutputs,nNewInput)
        newWeightNext = nn.init.xavier_uniform_(torch.empty(self.nOutputs, nNewInput))
        self.network.linearOutput.weight.data = torch.cat((self.network.linearOutput.weight.data,newWeightNext),1)
        del self.network.linearOutput.weight.grad

        self.network.linearOutput.in_features = nNewInputCurr
        self.netUpdateProperties()
        
    def inputPruning(self,pruneIdx,nPrunedNode = 1):
        nNewInputCurr = self.nNetInput - nPrunedNode

        # prune input weight of next layer
        self.network.linearOutput.weight.data = deleteColTensor(self.network.linearOutput.weight.data,pruneIdx)
        del self.network.linearOutput.weight.grad

        # update input features
        self.network.linearOutput.in_features = nNewInputCurr
        self.netUpdateProperties()


class DEVDAN():
    def __init__(self,nInput,nOutput,LR = 0.02):
        # initial network
        self.net = [hiddenLayer(nInput,nOutput),outputLayer(nOutput,nOutput)]

        # network significance
        self.averageBias  = meanStdCalculator()
        self.averageVar   = meanStdCalculator()
        self.averageInput = meanStdCalculator()

        self.averageBiasGen = [meanStdCalculator()]
        self.averageVarGen  = [meanStdCalculator()]

        # hyper parameters
        self.lr           = LR
        self.criterion    = nn.CrossEntropyLoss()
        self.criterionGen = nn.MSELoss()            # loss for generative loss = 0.5*criterion(scores,minibatch_label)
        
        # Evolving
        self.growNode   = False
        self.pruneNode  = False

        # properties
        self.nHiddenLayer = 1
        self.nHiddenNode  = nOutput
        self.nOutputs     = nOutput
        self.winLayerIdx  = 0
        
    def updateNetProperties(self):
        self.nHiddenLayer = len(self.net) - 1
        nHiddenNode = 0
        for iLayer in range(0,len(self.net)-1):
            nHiddenNode += self.net[iLayer].nNodes
        self.nHiddenNode = nHiddenNode

    def getNetProperties(self):
        for iLayer,nett in enumerate(self.net):
            print('\n',iLayer + 1,'-th layer')
            nett.getNetProperties()
        
    # ============================= Evolving mechanism =============================
    def hiddenNodeGrowing(self,layerIdx = -2):
        if layerIdx <= (len(self.net)-1):
            copyHiddenLayer = copy.deepcopy(self.net[layerIdx])
            copyHiddenLayer.nodeGrowing()
            self.net[layerIdx] = copy.deepcopy(copyHiddenLayer)

            if layerIdx == -2:
                # grow input for classifier
                copyOutputLayer = copy.deepcopy(self.net[layerIdx+1])
                copyOutputLayer.inputGrowing()
                self.net[-1] = copy.deepcopy(copyOutputLayer)
            else:
                # grow input for classifier
                copyNextNet = copy.deepcopy(self.net[layerIdx+1])
                copyNextNet.inputGrowing()
                self.net[layerIdx+1] = copy.deepcopy(copyNextNet)
        
            # print('+++ GROW a hidden NODE +++')
            self.updateNetProperties()
        else:
            raise IndexError
        
    def hiddenNodePruning(self,layerIdx = -2):
        if layerIdx <= (len(self.net)-1):
            copyHiddenLayer = copy.deepcopy(self.net[layerIdx])
            copyHiddenLayer.nodePruning(self.leastSignificantNode)
            self.net[layerIdx] = copy.deepcopy(copyHiddenLayer)

            if layerIdx == -2:
                # prune input for classifier
                copyOutputLayer = copy.deepcopy(self.net[layerIdx+1])
                copyOutputLayer.inputPruning(self.leastSignificantNode)
                self.net[-1] = copy.deepcopy(copyOutputLayer)
            else:
                # prune input for next layer
                copyNextNet = copy.deepcopy(self.net[layerIdx+1])
                copyNextNet.inputPruning(self.leastSignificantNode)
                self.net[layerIdx+1] = copy.deepcopy(copyNextNet)
        
            # print('+++ GROW a hidden NODE +++')
            self.updateNetProperties()
        else:
            raise IndexError

    # ============================= forward pass =============================
    def feedforwardTest(self,x,device = torch.device('cpu')):
        # feedforward to all layers
        with torch.no_grad():
            tempVar = x.to(device)
            tempVar = tempVar.type(torch.float)
            
            for iLayer in range(len(self.net)):
                currnet = self.net[iLayer].network
                obj     = currnet.eval()
                obj     = obj.to(device)
                if iLayer == len(self.net)-1:
                    # get output
                    tempVar   = obj(tempVar)
                else:
                    tempVar,_ = obj(tempVar)

            self.scoresTest            = tempVar
            self.multiClassProbability = F.softmax(tempVar.data,dim=1)
            self.predictedLabelProbability, self.predictedLabel = torch.max(self.multiClassProbability, 1)

    def feedforwardTrainDiscriminative(self,x,device = torch.device('cpu')):
        # feedforward to the winning layer
        tempVar = x.to(device)
        tempVar = tempVar.type(torch.float)
        
        # feedforward to all layers
        for iLayer in range(len(self.net)):
            currnet = self.net[iLayer].network
            obj     = currnet.train()
            obj     = obj.to(device)
            if iLayer == len(self.net)-1:
                # get output
                tempVar   = obj(tempVar)
            else:
                tempVar,_ = obj(tempVar)

        self.scoresTrain = tempVar

    def feedforwardBiasVarDiscriminative(self,x,label_oneHot,device = torch.device('cpu')):
        # label_oneHot is label in one hot vector form, float, already put in device
        with torch.no_grad():
            tempVar = x.to(device)
            tempVar = tempVar.type(torch.float)
            
            hiddenNodeSignificance = []

            for iLayer in range(len(self.net)):
                currnet           = self.net[iLayer].network
                obj               = currnet.eval()
                obj               = obj.to(device)
                
                if iLayer == 0:
                    tempVar,_  = obj(tempVar)
                    tempVar2   = (tempVar.clone().detach())**2

                    # node significance
                    hiddenNodeSignificance.append(tempVar.clone().detach().squeeze(dim=0).tolist())

                else:
                    if iLayer == len(self.net)-1:
                        # get output
                        tempVar  = obj(tempVar)
                        tempVar2 = obj(tempVar2)
                    else:
                        tempVar,_  = obj(tempVar)
                        tempVar2,_ = obj(tempVar2)

                    if iLayer < len(self.net) - 1:
                        # node significance 
                        hiddenNodeSignificance.append(tempVar.clone().detach().squeeze(dim=0).tolist())
                    
            # bias variance
            tempY    = F.softmax(tempVar,dim=1)                 # y
            tempY2   = F.softmax(tempVar2,dim=1)                # y2
            bias     = torch.norm((tempY - label_oneHot)**2)    # bias
            variance = torch.norm(tempY2 - tempY**2)            # variance

            self.bias     = bias.item()
            self.variance = variance.item()
            self.hiddenNodeSignificance = hiddenNodeSignificance

    # ============================= Network Evaluation =============================
    def updateBiasVariance(self):
        # calculate mean of bias
        # should be executed after doing feedforwardBiasVar on the winning layer
        self.averageBias.updateMeanStd(self.bias)
        if self.averageBias.count < 1 or self.growNode:
            self.averageBias.resetMinMeanStd()
        else:
            self.averageBias.updateMeanStdMin()
        
        # calculate mean of variance
        self.averageVar.updateMeanStd(self.variance)
        if self.averageVar.count < 20 or self.pruneNode:
            self.averageVar.resetMinMeanStd()
        else:
            self.averageVar.updateMeanStdMin()

    def growNodeIdentification(self):
        dynamicKsigmaGrow = 1.25*np.exp(-self.bias) + 0.75
        growCondition1    = (self.averageBias.minMean + 
                             dynamicKsigmaGrow*self.averageBias.minStd)
        growCondition2    = self.averageBias.mean + self.averageBias.std

        if growCondition2 > growCondition1 and self.averageBias.count >= 1:
            self.growNode = True
        else:
            self.growNode = False
    
    def pruneNodeIdentification(self, layerIdx = -2):
        dynamicKsigmaPrune = 1.25*np.exp(-self.variance) + 0.75
        pruneCondition1    = (self.averageVar.minMean + 
                              2*dynamicKsigmaPrune*self.averageVar.minStd)
        pruneCondition2    = self.averageVar.mean + self.averageVar.std
        
        if (pruneCondition2 > pruneCondition1 and not self.growNode and 
            self.averageVar.count >= 20 and
            self.net[layerIdx].nNodes > self.nOutputs):
            self.pruneNode = True
            self.findLeastSignificantNode(layerIdx)
        else:
            self.pruneNode = False

    def findLeastSignificantNode(self,layerIdx = -1):
        # find the least significant node in the winning layer
        # should be executed after doing feedforwardBiasVar on the winning layer
        self.leastSignificantNode = torch.argmin(torch.abs(torch.tensor(self.hiddenNodeSignificance[layerIdx]))).tolist()

    # ============================= Training ============================= 
    def discriminativeTraining(self,device = torch.device('cpu'),batchSize = 1,epoch = 1):
        # shuffle the data
        nData            = self.batchData.shape[0]
        
        # label for bias var calculation
        y_biasVar = F.one_hot(self.batchLabel, num_classes = self.net[-1].nOutputs).float()
        
        for iEpoch in range(0,epoch):

            shuffled_indices = torch.randperm(nData)

            for iData in range(0,nData,batchSize):
                # load data
                indices                 = shuffled_indices[iData:iData+batchSize]

                minibatch_xTrain         = self.batchData[indices]
                minibatch_xTrain         = minibatch_xTrain.to(device)
                minibatch_xTrain_biasVar = minibatch_xTrain

                minibatch_labelTrain     = self.batchLabel[indices]
                minibatch_labelTrain     = minibatch_labelTrain.to(device)
                minibatch_labelTrain     = minibatch_labelTrain.long()

                if iEpoch == 0:
                    minibatch_label_biasVar = y_biasVar[indices]
                    minibatch_label_biasVar = minibatch_label_biasVar.to(device)
                    
                    if batchSize > 1:
                        minibatch_xTrain_biasVar = torch.mean(minibatch_xTrain,dim=0).unsqueeze(dim=0)
                        minibatch_label_biasVar  = torch.mean(minibatch_label_biasVar,dim=0).unsqueeze(dim=0)

                    # calculate mean of input
                    # self.averageInput.updateMeanStd(torch.mean(minibatch_xTrain,dim=0).unsqueeze(dim=0))
                    self.averageInput.updateMeanStd(minibatch_xTrain_biasVar)

                    # get bias and variance
                    # outProbit = probitFunc(self.averageInput.mean,self.averageInput.std)   # for Sigmoid activation function
                    # self.feedforwardBiasVar(outProbit,minibatch_label_biasVar)             # for Sigmoid activation function
                    self.feedforwardBiasVarDiscriminative(self.averageInput.mean,minibatch_label_biasVar)  # for ReLU activation function

                    # update bias variance
                    self.updateBiasVariance()

                    # growing
                    self.growNodeIdentification()
                    if self.growNode:
                        self.hiddenNodeGrowing(self.winLayerIdx)

                    # pruning
                    if not self.growNode:
                        self.pruneNodeIdentification(self.winLayerIdx)
                        if self.pruneNode:
                            self.hiddenNodePruning(self.winLayerIdx)

                # declare parameters to be trained
                self.getTrainableParameters()

                # specify optimizer
                optimizer = torch.optim.SGD(self.netOptim, lr = self.lr, momentum = 0.95,  weight_decay = 0.00005)

                # forward pass
                self.feedforwardTrainDiscriminative(minibatch_xTrain)
                loss = self.criterion(self.scoresTrain,minibatch_labelTrain)

                # backward pass
                optimizer.zero_grad()
                loss.backward()

                # apply gradient
                optimizer.step()

    # def generativeTraining
                

    def trainingDataPreparation(self, batchData, batchLabel, activeLearning = False,
        advSamplesGenrator = False, minorityClass = None):

        if activeLearning:
            # sample selection
            # MCP: multiclass probability
            sortedMCP,_          = torch.sort(self.MultiClassProbability, descending=True)
            sortedMCP            = torch.transpose(sortedMCP, 1, 0)
            sampleConfidence     = sortedMCP[0]/torch.sum(sortedMCP[0:2], dim=0)
            indexSelectedSamples = sampleConfidence <= 0.75
            indexSelectedSamples = (indexSelectedSamples != 0).nonzero().squeeze()

            # selected samples
            batchData  = batchData[indexSelectedSamples]
            batchLabel = batchLabel[indexSelectedSamples]
            # print('selected sample size',batchData.shape[0])

        # training data preparation
        # there is no buffer data, no drift, no warning
        self.batchData  = batchData
        self.batchLabel = batchLabel
        # if self.driftStatus == 0 or self.driftStatus == 2:  # STABLE or DRIFT
        #     # check buffer
        #     if self.bufferData.shape[0] != 0:
        #         # add buffer to the current data batch
        #         self.batchData  = torch.cat((self.bufferData,batchData),0)
        #         self.batchLabel = torch.cat((self.bufferLabel,batchLabel),0)

        #         # clear buffer
        #         self.bufferData  = torch.Tensor().float()
        #         self.bufferLabel = torch.Tensor().long()
        #     else:
        #         # there is no buffer data
        #         self.batchData  = batchData
        #         self.batchLabel = batchLabel

        #     # provide data category for original samples
        #     # 0: original samples; 1: anomaly samples; 2: augmented samples
        #     nOriginalData       = self.batchData.shape[0]

        #     if self.driftStatus == 2 and self.anomalyDataNadine.anomalyData.shape[0] != 0:
        #         # check anomaly data if drift
        #         # add anomaly data to the current data batch
        #         nAnomalyData    = self.anomalyDataNadine.anomalyData.shape[0]
        #         self.batchData  = torch.cat((self.anomalyDataNadine.anomalyData, self.batchData),0)
        #         self.batchLabel = torch.cat((self.anomalyDataNadine.anomalyLabel,self.batchLabel),0)
        #         # print('$$$ Anomaly data is added to the training set. Number of data: ',self.batchData.shape[0],'$$$')
        #         self.anomalyDataNadine.reset()

        #         # provide data category for anomaly data
        #         # 0: original samples; 1: anomaly samples; 2: augmented samples

        # if self.driftStatus == 1:  # WARNING
        #     # store data to buffer
        #     # print('Store data to buffer')
        #     self.bufferData  = batchData.clone().detach()
        #     self.bufferLabel = batchLabel.clone().detach()

        # generate adversarial samples for minority class
        # if advSamplesGenrator and (self.driftStatus == 0 or self.driftStatus == 2):
        if advSamplesGenrator:
            # prepare data
            if minorityClass is not None:
                # select the minority class data
                adversarialBatchData  = self.batchData [self.batchLabel == minorityClass]
                adversarialBatchLabel = self.batchLabel[self.batchLabel == minorityClass]

                nMinorityClass = adversarialBatchData.shape[0]
                nMajorityClass = self.batchData.shape[0] - nMinorityClass
            else:
                # select all data
                adversarialBatchData  = self.batchData.clone().detach()
                adversarialBatchLabel = self.batchLabel.clone().detach()

            # forward pass
            adversarialBatchData.requires_grad_()
            self.feedforwardTrain(adversarialBatchData)
            lossAdversarial = self.criterion(self.scoresTrain,adversarialBatchLabel)

            # backward pass
            lossAdversarial.backward()

            # get adversarial samples
            adversarialBatchData = adversarialBatchData.clone().detach() + 0.007*torch.sign(adversarialBatchData.grad)

            self.batchData  = torch.cat((self.batchData,adversarialBatchData),0)
            self.batchLabel = torch.cat((self.batchLabel,adversarialBatchLabel),0)

            # provide data category for augmented data
            # 0: original samples; 1: anomaly samples; 2: augmented samples
            nAdversarialSamples = adversarialBatchData.shape[0]
            # print('selected sample size',self.batchData.shape[0])

    def getTrainableParameters(self):
        for iLayer in range(len(self.net)):
            netOptim  = []
            netOptim  = netOptim + list(self.net[iLayer].network.parameters())
        
        self.netOptim = netOptim

    # ============================= Testing ==============================
    def testing(self,x,label,device = torch.device('cpu')):
        # load data
        x     = x.to(device)
        label = label.to(device)
        label = label.long()
        
        # testing
        start_test          = time.time()
        self.feedforwardTest(x)
        end_test            = time.time()
        self.testingTime    = end_test - start_test
        
        loss                = self.criterion(self.scoresTest,label)
        self.testingLoss    = loss.detach().item()
        correct             = (self.predictedLabel == label).sum().item()
        self.accuracy       = 100*correct/(self.predictedLabel == label).shape[0]  # 1: correct, 0: wrong
        self.trueClassLabel = label